무중단 배포가 구체적으로 어떻게 수행되는지, 또 고민 포인트는 무엇인지 찾아보는 중. <br>
[Graceful Shutdown과 함께 보기](https://github.com/binary-ho/TIL-public/blob/main/Spring/Graceful%20Shutdown.md)

<br>


# 1. Rolling Update
신규 버전 생성, 트래픽을 옮기고 기존 버전 내리고 이런식
쿠버에서 기본으로 제공해준다.

## Kubernetes Rolling Deployment 과정

![Image](https://github.com/user-attachments/assets/2b14f0d8-baa4-4638-a72c-036b9e64ba26)

<br>

두가지 옵션 설정이 가능하다.
1. `maxUnavilable`: (몇개까지 내려가 있을 수 있는가?) 동시에 "사용 불가" 상태가 될 수 있는 Pod 갯수 설정 가능
2. `maxSurge`: 전체 목표 갯수보다 몇개나 임시로 더 띄울 수 있는지?

예를 들어 Replica 3개, maxSurge 1개, maxUnavailable 1개일 때, 아래와 같이 진행된다. <br>

1. Pod 생성 및 구동 (maxSurge 만큼)
2. 준비상태(Readiness Probe), 활성상태(Liveness Probe) 등을 확인해 Pod 내부 애플리케이션 정상 구동 확인. (pod Ready 상태는 실제로 트래픽 받을 수 있는 시점보다는 더 빠를 수 있으니, 준비상태를 확인)
3. Health Check -> Ready 상태로 변경. (Ready 상태가 되기 전엔 실제 트래픽이 전달되지 않음)
4. 이후 v1 Pod 하나를 내린다.
5. 내린 Pod는 Service Endpoint에서 제거되어 트래픽을 받지 못해, 설사 종료되지 않더라도 문제 X

<br>

결론: 추가하고, 내리고, 추가하고, 내리고

### Rolling Update 문제점
1. 배포를 중간에 멈출 수 없음
   - **신규 버전에 문제가 발생해도, 배포를 멈출 수 없으므로 그대로 장애 발생**
   - **중간에 멈출 수 없기에, 기존 버전으로 롤백하는 것도 고통스럽다.**
즉, 적용은 간단하지만 장애시 영향을 받는 사용자도 많고, 복구 시간이 길다. <br>
-> 문제 해결을 위해 Canary를 사용할 수 있다.

<br>

2. 버전이 맞지 않는 문제
   - 배포중 서비스를 사용하는 유저가 발생시킨 요청이 전부 한 버전으로만 발생하지 않을 수 있다.
   - 예를 들어 클라이언트 웹 페이지는 v1으로 가져왔는데, 웹 페이지 구성을 위한 번들 요청은 v2로 보내는 경우 문제가 발생할 수 있음.
이게 큰 문제다. <br>
결국 새로운 버전이 배포된 이후, 강제로 유저를 내쫗고 다시 들어오게 하지 않는 이상 발생하는 문제. (클라의 경우 강제 새로고침?) <br>
어떻게 해결해볼 수 있을까... 더 아래 "4. 버전 문제"에서 고민

3. 배포가 느리다: 순차적으로 배포하므로..


<br>

# 2. Blue/Green

![Image](https://github.com/user-attachments/assets/c453bcea-8c21-4e04-8352-f6ed1c00b6d8)


<br>

새로운 버전의 인스턴스를 미리 준비시켜둔 다음에 한번에 트래픽을 옮기는 배포 방식이다.
"green 배포"를 생성한 이후, 트래픽을 새로운 버전으로 옮기면 된다.

### 장점
1. Deployments are fast: 새로운 버전은 트래픽을 전혀 받지 않고, 원하는 갯수의 인스턴스를 전부 한번에 실행 시키기 때문에 배포 프로세스가 빠름
2. Rollbacks are painless: 문제가 생길 시, 단지 이전 버전들로 트래픽을 모두 옮기면 되므로 롤백이 painless하다.

### 단점
1. 2배의 production 환경 자원 필요: 동일한 prod 환경이 2배로 필요하다. 동일한 인스턴스 갯수의 두 버전을 동시에 실행할 수 있어야 하므로..
2. (이해 조금 안됨) 3가지 버전이 동시에 존재할 수 있고, 이럴 때 어떻게 해야할지 판단하기 어렵다. 예를 들어 거의 배포가 완료된 신버전과 구버전이 동시에 돌아가고 있는데, 또 새로운 버전을 배포하려 한다면?

# 3. Canary 배포 방식
새로운 버전에 트래픽 비율을 점진적으로 늘려가는 방식. 네트워크 레벨에서 트래픽을 조절하며 배포가 이루어진다. <br>
모니터링 후 문제 발생 시 즉시 롤백할 수 있다. (롤링 업데이트 단점 보완)

- 사례: 토스 뱅크에서는 대고객 서비스엔 카나리 배포를 주로 활용하고, 트래픽이 적거나 없으면 롤링 업데이트 방식 사용

## 3.2 Canary 배포의 문제점
1. 신규 버전과 구버전 간에 호응이 안될 수 있다.
    - 신규 버전에선 기능이 변경되어, 구버전에선 404 에러 발생할 수도
    - 예를 들어 신규 버전에서 새로운 탭이 생겨, 고객이 신규 탭을 눌렀는데, 신규 탭에서 발생한 Request가 구버전 API 서버에 요청되는 경우 404가 뜰 수 있다.
2. 이벤트에 영원히 참여하지 못하는 상황

# 4. API 버전 문제의 해결
1. 버전이 달라도 문제 없게 만든다.
   - 클라이언트의 경우 빌드 결과물인 정적 번들을 FE 서버가 아닌 별도 CDN에서 가져오는 식의 시도를 할 수도 있다.
   - Feature Flag, 메타 프로그래밍 등 활용할 수도 있다.
     - [11번가 Feature Flag 도입](https://youtu.be/8EZZQZJTKEk?si=PIbT0WG0n53Y7cRA)
     - [당근](https://www.youtube.com/watch?v=wQIjeVyVU5s)
2. 한 유저의 요청을 모두 한 버전으로만 향하게 한다.
    - Sticky Canary를 통해 버전을 고정하는 예시 (토스뱅크)

## 4. Sticky Canary
스티키 세션과 비슷한 아이디어로, 동일 유저 호출은 동일 버전으로 고정하는 전략. <br>
유저 경험을 안정적으로 유지 가능

- 사례: 요청에 대해 유니크한 키를 부여하고, 그 결과를 활용해 고정 버전으로 요청이 향하게 한다.
- 컨시스턴스 해시 라우팅 계층을 추가해, 카나리에서 트래픽을 나누는 비율과 동일한 비율로 라우팅 해주는 방법도 있겠지만, 카나리 배포 특성상 트래픽 분배 비율이 계속 바뀌기 때문에, 컨시스턴스 해시에서도 wight가 계속 바뀌게 되고 결국 똑같이 다른 버전을 호출할 수도 있음.
- [영상 다시 보고 더 공부하자](https://youtu.be/ApNj9MZU7Ak)



- [은행 최초 코어뱅킹 MSA 전환기 (feat. 지금 이자 받기)](https://toss.tech/article/22563)
- [생산성과 안정성 모두 잡는 마스터키, Canary 배포 개선기](https://youtu.be/ApNj9MZU7Ak)
- [[FE] Rolling Update 기반의 웹 애플리케이션 배포에서 발생하는 문제점과 Blue/Green 배포](https://junglast.com/blog/k8s-blue-green-deployment)
- [[Kubernetes Practical Lab] Rolling Update Project](https://medium.com/@kylelzk/kubernetes-practical-lab-rolling-update-project-f69b418b2fe6)
- [Scalability concepts: zero-downtime deployments](https://avikdas.com/2020/06/30/scalability-concepts-zero-downtime-deployments.html)