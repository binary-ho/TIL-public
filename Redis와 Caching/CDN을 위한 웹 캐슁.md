# CDN을 위한 웹 캐싱
# 1. 웹 캐싱이란?
캐싱이란 저장 장치 계층간의 속도 차이를 완중하기 위한 기법으로, <br>
캐쉬 메모리, 페이징, 버퍼링 등이 그 예시이다. <br> <br>

웹 사이트에 대한 폭발적인 수요로 인해 <br>
기존의 하드웨어 내에서 수행하던 캐싱이나, 단일 시스템 내에서 속도차가 있는 저장장치간 캐싱 기법 뿐 아니라, 원격지의 객체를 캐싱하는 기법의 중요성이 커지고 있다. <br>

뭐 사실 온갖 웹-앱 기반 서비스에 다 캐싱이 쓰이지만, SNS를 생각해보자. 캐싱이 없다면 징그럽게도 많은 DB I/O가 있을 것이다. <br>
인스타그램은 10억명의 활성 사용자를 기록한 적이 있다;; 이만큼의 인원이 들어올 때마다 피드를 가져오는 쿼리를 날린다고 상상해보면 얼마나 끔찍한 일인지 알 수 있다. <br>
쓰기 보다 요청이 압도적으로 많은 거대 기업들은 기꺼이 원본 데이터와의 일치를 약간 포기하고 웹 객체를 캐싱한다 <br>
기꺼이까지는 아니지만, Facebook에서도 Memcache를 사용중이다 일관성을 위해 최선을 다하겠지만 가용성이 더 중요하다고 Scaling Memcache  at Facebook라는 글에서 언급했다. <br> <br>

여하튼 웹 캐싱은 사용자에 의해 빈번히 요청되는 컨텐츠를 캐싱해서 사용자에게 더 빨리 제공하겠다는 목표를 가지고 있고, <Br> 
CDN의 경우 사용자와 지리적으로 가까운 웹 캐쉬에 보관해 빠른 서비스를 보장한다. <br>

![image](https://github.com/binary-ho/TIL-public/assets/71186266/1182ad13-80a4-46e3-b1c8-51d35d242dc6)

나도 프로젝트 아임히어에서 ([출석 서비스 아임 히어!](https://imhere.im)) CDN 서비스를 사용하고 있다. <br>
위의 그림에서 가장 오른쪽을 보면, Amazon S3라고 적혀있는 구름 그림이 있는데, S3이라는 저장소(버킷)에 js로 작성된 내 정적 파일들을 올려두면, <br>
CloudFront라는 CDN 서비스가 중간에서 fetch시간을 줄여준다. 가운데 네모인 Edge Location이 CloudFront의 위치가 되겠다. <br>
(그 외에도 사용 이유가 있지만 글의 주제와는 벗어나서 PASS) <br>
<br>

이러한 웹 캐싱은 우리가 OS 시간에 배웠던 기존의 캐싱과는 성격이 조금 다르다. <br>
어떻게 다른지 그리고 어떤 특징이 있는지 알아보자

# 2. 기존의 캐싱과 웹 캐싱의 차이?
**페이징**과 같은 기존의 캐싱들은 사용자가 요청한 객체가 캐싱에 있기만 하면 그 효용이 모두 동일했다. <br>
그게 무슨 말인고 하니, 페이지들은 모두 크기가 같고, 디스크에서 페이지를 가져오는 비용은 동일하다. <br>
결국 참조 어떤 객체를 캐시에 남겨두고, 버릴지를 고민할 때 그저 참조 가능성이 높은 것을 남겨 hit ratio를 높히는 것이 중요하다. <br> <br>

하지만, 웹 캐싱 상황에서 캐싱되는 객체들의 크기나, 인출 비용이 다르다. <br>
예를 들면 뭐 나는 출석 서비스에서 수업이 OPEN되는 경우 학생들의 빠른 출석을 위해 수업 정보와 수강생 정보를 저장한다. <br>
수업 정보는 단순히 OpenLecture라는 String형 필드 4개 정도를 가진 객체가 Hash형태로 저장된다. 가볍다 <br>
이에 비해 수강생 정보는 단순 String이지만 한번에 수백명이 수강하기 때문에 수백개의 Stirng으로 이루어져 있다. <br>
이렇게 웹 캐싱의 경우 우리가 "페이지"를 캐싱했을 때와는 달리 객체의 크기가 다르다. <br> <br>

<br>

그리고 인출 비용이 다르다는 것은 특히 분산환경의 경우 서버마다 가져오는 비용이 다를 수 있다. <br>

![image](https://github.com/binary-ho/TIL-public/assets/71186266/b1619bc4-7b08-4900-9187-fd85331332e4)

<br>

적절한 예시인지는 잘 모르겠지만, 회원 데이터는 Postgresql가 올라간 AWS RDS에서 바로 가져오고, <br>
어떤 데이터는 멀리 있고 느린 서버와 DB를 거쳐서 데이터를 가져와야 한다고 생각해보자. <br>
이런 경우 데이터를 인출하는데 걸리는시간이 다를 수 밖에 없다는 것이다. <br>
즉, 웹 캐싱에선, 캐싱할 대상의 '크기'와 '인출 비용'이 이질적이다! <br> <br>

따라서, 웹 캐싱에서는 단순히 hit ratio보다도
비용이 얼마나 감소되었는지, **즉 "비용 절감률"이라는 요소가 더욱 중요하다!!** <Br> 

# 3. 캐싱의 가치 평가와 비용

왜 웹 캐싱에선 Hit Ratio보다 비용 절감률이 중요할까? <br>
예를 들어서 자세히 설명해보겠다. <br> <br>

1초 동안 9번 참조되는 데이터 A와 1번 참조되는 데이터 B가 있다. <br>
위의 그림에서 같은 AZ 안에 있는 데이터가 A, 느림보 친구들을 거친 데이터를 B이다. <br>
A를 가져오는데 걸리는 시간을 1ms라고 가정하고,
B를 가져오는 시간을 100ms라고 가정하겠다. <br>
그리고 아픈 이런 저런 사정으로 인해 둘 중에 하나만 딱 캐싱할 공간이 남은 것이다 <br>

1. A : 초당 참조 횟수 9회, 인출 시간 1ms
2. B : 초당 참조 횟수 1회, 인출 시간 500ms

단순하게 cache hit만 따진다면 A를 저장하는 것이 적절하다. <br>
A를 캐싱해둔다면 1초 동안 cache hit ratio는 무려 90%나 된다. (팀장님 저 잘 했죠~?? 할 수 있다) <br> <br>

그러나 지연 시간은 각각 아래와 같다. 
1. A를 캐싱할 때 지연 시간 500ms (B의 부재로 인해)
2. B를 캐싱할 때 지연 시간 9ms (A의 부재로 인해)

<br>

캐시에서 데이터를 가져오는 시간은 극도로 작아 무시한다고 생각한다. <br>
위와 같은 상황은 충분히 있을 수 있다. 이질 분산 환경에서의 캐싱은 단순히 Hit Ratio 보다도 얼마나 비용을 절감했는지가 중요하다. <br>

# 4. 비용의 종류와 평가 방식
비용 감소율을 따질 때의 비용은 캐싱의 목적에 따라 다르게 계산해주면 된다. <br>
지연 시간을 줄이기 위해 캐싱했다면 "지연 감소율"이 비용 절감률이고, "용량"이 중요한 경우 "바이트 적중률"이 비용 절감률이다. <br>
결국, 캐싱으로 인해 목적을 얼마나 달성했는가, 문제 상황을 얼마나 개선했는가가 궁금한게 당연하기 때문이다. <br>
물론 단순히 적중 횟수를 높히는 것이 중요하다면 Hit Ratio가 비용 절감률이 된다. <br> <Br>


이러한 비용 감소율은 간단하게 아래와 같이 계산할 수 있다.
(수식)

<br>

많은 비약을 넣어 심플하게 생각하자면, `(이전 비용 - 현재 비용) / 이전 비용`으로 계산해 주면 된다. <br> <br>

재미있다. 기존 의 "동질형" 캐싱 상황에서는 시간 지역성과 참조 인기도를 고려해 캐싱하면 됐지만, <br>
웹 캐싱에선 고려할 거리가 더욱 많은 것이다.

# 5. 웹 캐싱 교체 알고리즘

큰 문제가 하나 있다. <br>
캐시은 보통 비싼 자원에 저장되고, 그 크기가 한정되어 있다. <Br>
그렇기에 캐싱 교체 알고리즘은 캐싱 최대 효율을 위해 분주하게 교체 알고리즘을 통해 누굴 남기고 누굴 내보낼지 결정해야 한다. <Br> 
기존 동질형 캐싱 상황에서도 이는 안 그래도 어려운 문제였는데, 이질형 캐싱에선 더욱 끔찍해진다 <br> <br>

캐싱에서는 모든 미래 참조를 알고 있다고 가정하는 알고리즘이 있다 -> TODO
웹 캐싱에서는 모든 미래 참조를 알더라도 최적의 알고리즘을 찾는 것이 NP-Hard로 알려져있다. <br>


이러한 웹 캐싱은 다음의 두 경우에서 효과적이다.


# 6. 웹 캐시의 <span style="color:#ffc000">일관성 유지 기법</span>
   
웹 캐시 환경에선 일반적인 캐시 환경과 다르게, <span style="color:#ffc000">캐시의 부재가 시스템에 치명적인 문제로 작용하지는 않는다.</span>
그렇기 떄문에, 보통은 엄격한 방법 보다는 <span style="color:#ffc000">약한 일관성 유지 기법</span>(weak consistency)을 사용한다.

1. <span style="color:#ffc000">Polling-Every-Time</span> : 캐시에 존재하는 객체에 대한 요청이 있을 때마다, 근원지 서버에 객체의 변경 여부를 확인한다.
2. <span style="color:#ffc000">Invalidation </span>: 무효화 기법. 근원 서버가 자신의 객체를 캐싱하고 있는 모든 프락시 서버를 기록해 두었다가 해당 객체가 변경되는 경우 프락시 서버들에게 변경 사실을 알린다.
3. <span style="color:#ffc000">Adaptive TTL</span> : 최종 변경 시각과, 최종 확인 시각을 고려해서 <span style="color:#ffc000">변경되었을 가능성이 높다고 판단되는 경우에 변경 여부를 확인한다.</span> LMF (Last Modified Factor) 값이 임계치를 넘어가면 변경 여부를 확인한다.

![[Pasted image 20230920105147.png]]

웹 캐시 Prefething
   
웹 서비스의 응답 지연 시간을 줄이기 위한 시도로, 두 가지 종류가 있다.
1. <span style="color:ffc000">예측 사전 인출</span> : 웹 페이지들 간의 관계 그래프 등을 구성해 하나의 웹 페이지가 참조되었을 떄 새로운 웹 페이지가 참조될 확률을 과거 참조 기록을 통해 예측하여 인출한다.
2. <span style="color:#ffc000">대화식 사전 인출</span> :  클라이언트가 HTML 문서 요청을 했을 때, 미리 파싱하여 문서에 포함되거나 연결된 웹 객체를 미리 받아와서, 후속 요청에 바로 전달하는 기법
3. <span style="color:#ffc000">유효성 사전 확인 </span>(일관성 유지 기법, prevalidation) : 일관성 유지시, 유효성을 미리 확인해 두고, 사용자 요청시 따로 확인하지 않음
교체 알고리즘을 구현하는 자료구조?

LRU와 같은 형태는 사실상 순서만이 중요하기 때문에, 단순 `List` 형태로도 가능하다. O(N)
새롭게 참조된 객체를 가장 앞에 두기만 하면 된다.

하지만, 보통은 더욱 복잡한 가치 평가가 필요하므로, `Heap` 이용해 O(log_2N) 으로 캐쉬 연산을 구현하는 편이다.

문제는 참조되지도 않았는데, 가치가 변하는 상황이 있다 (aging 등..)
이런 상황에서 어쩔 수 없이 O(n) 시간복잡도 알고리즘이 나오게 되는데, 보통 근사적인 구현 방법으로 시간 복잡도를 낮춘다.
